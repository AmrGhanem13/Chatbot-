# ğŸ§  AI Chat Assistant with LLaMA 3, FastAPI, and Streamlit

This project integrates a powerful AI assistant using the **LLaMA 3-70B** model (via [Together.ai](https://www.together.ai)), deployed through a FastAPI backend and an interactive Streamlit frontend. It includes:

- âœ… Chat history (last 15 messages)
- âœ… Dynamic Streamlit UI
- âœ… Ngrok tunneling for public access

---

## ğŸš€ Features

- ğŸ’¬ Chat with Metaâ€™s **LLaMA 3-70B** via Together API  
- ğŸ” Persistent chat memory (up to last 15 exchanges)  
- âš¡ FastAPI backend at `/llm/` endpoint  
- ğŸ–¥ï¸ Streamlit frontend with user-friendly interface  
- ğŸŒ Ngrok support for global access  
- ğŸ”’ Environment variables using `.env`  

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
pip install streamlit fastapi uvicorn python-dotenv together
```
## ğŸ“ Project Structure
- project/
- â”œâ”€â”€ app.py              # Streamlit frontend UI
- â”œâ”€â”€ fast.py             # FastAPI backend API
- â”œâ”€â”€ .env                # API key config
- â”œâ”€â”€ .gitignore          # Ignored files
- â”œâ”€â”€ requirements.txt    # Dependencies
- â””â”€â”€ README.md           # Documentation

## ğŸ› ï¸ Run the App

- Start the FastAPI backend using Uvicorn:

```bash
uvicorn fast:app --reload
```

- Then in a new terminal, run the Streamlit frontend:

```bash
streamlit run app.py
```

## ğŸ‘¨â€ğŸ’» Author
Amr Ghanem
- |Software Engineer | Data Scientist & AI Developer
- ğŸ“« Open to collaborations and contributions
